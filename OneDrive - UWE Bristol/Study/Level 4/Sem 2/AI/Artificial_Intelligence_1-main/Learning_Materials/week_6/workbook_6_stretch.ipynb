{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\" > <h1> Activity 4: Creating a test harness for comparing ML algorithms on a dataset</h1>\n",
    "<p> Now you have done some manual experimenting with different hyper-parameter values for algorithms, it's time to think about automating that process.</p>\n",
    "<p>Complete the cell below to create a method that: </p>\n",
    "<ul>\n",
    "    <li> Takes a train and test data  arrays as  parameters <br>\n",
    "        HINT: develop your code using train_x, test_x,train_y,test_y for the iris data from above</li>\n",
    "    <li> Runs your SimpleKNNClassifier with K={1,3,5,7,9} and stores the test accuracy for each <br>\n",
    "    HINT: you could use:\n",
    "        <ul>\n",
    "            <li>a for loop to run the algorithm with different settings k  for  the number of neighbours(K),</li>\n",
    "            <li> an <a href=https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/>f-string</a> e.g. <code>experiment_name= f'KNN_K={k}'</code> to create a meaningful name for each run </li>\n",
    "            <li>a   dictionary to store your results, where each experiment has the string <em>experiment_name</em> as the key and the accuracy as the value </li>\n",
    "            </ul>for this?</li>\n",
    "    <li> Runs a DecisionTreeClassifier with all the different combinations of hyper-parameters from activity 3<br>\n",
    "       HINT: You could do this in the same way as I've suggested above but with nested for-loops (one for each hyper-parameter) and a more complex python f-string to create the name (key), then store the results in the same dictionary.  </li>\n",
    "    <li> Reports the results and which algorithm-hyperparameter combination has the highest test accuracy</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color:black\"\">\n",
    "    <h3> Reminder: Storing data in python dictionaries and iterating through their contents</h3>\n",
    "    <p> Python dictionaries are a way of storing data that can be accessed via a key<br>\n",
    "for example: <code> my_dict= {'name':'jim','familyname':\"Smith\", 'job':'professor'}</code><br>\n",
    "<b>Keys are usually strings</b>, but the values associated with a key can be any type, including numbers.</p>\n",
    "\n",
    "<p> The following snippets of code might be useful to you - <b>after</b> you have edited them.</p>\n",
    "<p> Make a new code cell in the notebook, copy the snippets in and run it, then edit it as you need.</p>\n",
    "<p><pre style='background:lightbrown;colour:black'>    \n",
    "labels = ['a','b','a','c','a','d','b']\n",
    "indexes = [1,4,6]\n",
    "mydict={}\n",
    "<span style=\"color:green\">for</span> idx <span style=\"color:green\">in</span> indexes:\n",
    "    <span style=\"color:green\">if</span> labels[idx] <span style=\"color:green\">in</span> mydict.keys():\n",
    "        mydict[labels[idx]] += 1\n",
    "    <span style=\"color:green\">else</span>: #create a new dictionary entry if needed\n",
    "        mydict[labels[idx]] = 1\n",
    "<span style=\"color:green\">print</span>(f'mydict is {mydict}')\n",
    "\n",
    "leastvotes=99\n",
    "<span style=\"color:green\">for</span> key,val <span style=\"color:green\">in</span> mydict.items():\n",
    "    <span style=\"color:green\">if</span> val < leastvotes:\n",
    "        unpopular= key\n",
    "        leastvotes=val\n",
    "<span style=\"color:green\">print</span>(f'{unpopular}, {leastvotes}')\n",
    "    </pre></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN_K=1: 0.9400\n",
      "KNN_K=3: 0.9600\n",
      "KNN_K=5: 0.9800\n",
      "KNN_K=7: 0.9800\n",
      "KNN_K=9: 0.9600\n",
      "DT_depth=None_split=2_crit=gini: 0.9800\n",
      "DT_depth=None_split=2_crit=entropy: 0.9200\n",
      "DT_depth=None_split=4_crit=gini: 0.9400\n",
      "DT_depth=None_split=4_crit=entropy: 0.9000\n",
      "DT_depth=None_split=6_crit=gini: 0.9400\n",
      "DT_depth=None_split=6_crit=entropy: 0.9000\n",
      "DT_depth=2_split=2_crit=gini: 0.9000\n",
      "DT_depth=2_split=2_crit=entropy: 0.9000\n",
      "DT_depth=2_split=4_crit=gini: 0.9000\n",
      "DT_depth=2_split=4_crit=entropy: 0.9000\n",
      "DT_depth=2_split=6_crit=gini: 0.9000\n",
      "DT_depth=2_split=6_crit=entropy: 0.9000\n",
      "DT_depth=4_split=2_crit=gini: 0.9400\n",
      "DT_depth=4_split=2_crit=entropy: 0.9200\n",
      "DT_depth=4_split=4_crit=gini: 0.9400\n",
      "DT_depth=4_split=4_crit=entropy: 0.9000\n",
      "DT_depth=4_split=6_crit=gini: 0.9400\n",
      "DT_depth=4_split=6_crit=entropy: 0.9200\n",
      "DT_depth=6_split=2_crit=gini: 0.9400\n",
      "DT_depth=6_split=2_crit=entropy: 0.9400\n",
      "DT_depth=6_split=4_crit=gini: 0.9400\n",
      "DT_depth=6_split=4_crit=entropy: 0.9000\n",
      "DT_depth=6_split=6_crit=gini: 0.9400\n",
      "DT_depth=6_split=6_crit=entropy: 0.9200\n",
      "\n",
      "Best configuration: KNN_K=5 with accuracy 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN_K=1': np.float64(0.94),\n",
       " 'KNN_K=3': np.float64(0.96),\n",
       " 'KNN_K=5': np.float64(0.98),\n",
       " 'KNN_K=7': np.float64(0.98),\n",
       " 'KNN_K=9': np.float64(0.96),\n",
       " 'DT_depth=None_split=2_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=None_split=2_crit=entropy': np.float64(0.92),\n",
       " 'DT_depth=None_split=4_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=None_split=4_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=None_split=6_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=None_split=6_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=2_split=2_crit=gini': np.float64(0.9),\n",
       " 'DT_depth=2_split=2_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=2_split=4_crit=gini': np.float64(0.9),\n",
       " 'DT_depth=2_split=4_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=2_split=6_crit=gini': np.float64(0.9),\n",
       " 'DT_depth=2_split=6_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=4_split=2_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=4_split=2_crit=entropy': np.float64(0.92),\n",
       " 'DT_depth=4_split=4_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=4_split=4_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=4_split=6_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=4_split=6_crit=entropy': np.float64(0.92),\n",
       " 'DT_depth=6_split=2_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=6_split=2_crit=entropy': np.float64(0.94),\n",
       " 'DT_depth=6_split=4_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=6_split=4_crit=entropy': np.float64(0.9),\n",
       " 'DT_depth=6_split=6_crit=gini': np.float64(0.94),\n",
       " 'DT_depth=6_split=6_crit=entropy': np.float64(0.92)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "irisX = iris.data\n",
    "irisy = iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, \n",
    "                                                   test_size=0.33, \n",
    "                                                   stratify=irisy, \n",
    "                                                   random_state=42)\n",
    "class SimpleKNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            label = self._predict(row)\n",
    "            predictions.append(label)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Compute distances\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        # Get k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Majority vote\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common\n",
    "    \n",
    "def first_ml_test_harness(train_x:np.ndarray,train_y:np.ndarray,\n",
    "                          test_x:np.ndarray=None,test_y:np.ndarray=None):\n",
    "    \"\"\" code to compare supervised machine learning algorithms on a dataset\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if test_x is None or test_y is None:\n",
    "        test_x, test_y = train_x, train_y\n",
    "    \n",
    "    # Test SimpleKNNClassifier with different K values\n",
    "    for k in [1, 3, 5, 7, 9]:\n",
    "        knn = SimpleKNNClassifier(k=k)\n",
    "        knn.fit(train_x, train_y)\n",
    "        pred_y = knn.predict(test_x)\n",
    "        accuracy = np.mean(pred_y == test_y)\n",
    "        experiment_name = f'KNN_K={k}'\n",
    "        results[experiment_name] = accuracy\n",
    "    \n",
    "    # Test DecisionTreeClassifier with different hyper-parameters\n",
    "    for max_depth in [None, 2, 4, 6]:\n",
    "        for min_samples_split in [2, 4, 6]:\n",
    "            for criterion in ['gini', 'entropy']:\n",
    "                dt = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                          min_samples_split=min_samples_split,\n",
    "                                          criterion=criterion)\n",
    "                dt.fit(train_x, train_y)\n",
    "                pred_y = dt.predict(test_x)\n",
    "                accuracy = np.mean(pred_y == test_y)\n",
    "                experiment_name = f'DT_depth={max_depth}_split={min_samples_split}_crit={criterion}'\n",
    "                results[experiment_name] = accuracy\n",
    "    \n",
    "    # Report all results\n",
    "    for experiment, accuracy in results.items():\n",
    "        print(f\"{experiment}: {accuracy:.4f}\")\n",
    "    \n",
    "    best_experiment = max(results, key=results.get)\n",
    "    best_accuracy = results[best_experiment]\n",
    "    print(f\"\\nBest configuration: {best_experiment} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "first_ml_test_harness(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jim_0 : 46\n",
      "nathan_0 : 79\n",
      "jim_1 : 23\n",
      "nathan_1 : 13\n",
      "jim_2 : 69\n",
      "nathan_2 : 48\n",
      "jim_3 : 12\n",
      "nathan_3 : 63\n",
      "jim_4 : 78\n",
      "nathan_4 : 39\n",
      " mydict.values() returns dict_values([46, 79, 23, 13, 69, 48, 12, 63, 78, 39]) of type <class 'dict_values'>\n",
      " biggest value is 79\n",
      "dict_keys(['jim_0', 'nathan_0', 'jim_1', 'nathan_1', 'jim_2', 'nathan_2', 'jim_3', 'nathan_3', 'jim_4', 'nathan_4'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#some names\n",
    "people= ['jim', 'nathan']\n",
    "RUNS= 5\n",
    "#some randomly created scores\n",
    "scores= [[46,23,69,12,78], [79, 13,48,63,39]]\n",
    "\n",
    "#some example of making keys and putting things into  dict\n",
    "mydict={}\n",
    "#populate mydict with items created using the data above\n",
    "for iteration in range(RUNS):\n",
    "    for person_idx in range(2):\n",
    "        name= f'{people[person_idx]}_{iteration}'\n",
    "        mydict[name]= scores[person_idx][iteration]\n",
    "\n",
    "#some examples of accessing a dictionary's contents\n",
    "for key,val in mydict.items():\n",
    "    print( f'{key} : {val}')\n",
    "\n",
    "# print all scores\n",
    "vals= mydict.values()\n",
    "print(f' mydict.values() returns {vals} of type {type(vals)}')\n",
    "\n",
    "# Have to cast vals into a list, then make a numpy array from that, to do maths\n",
    "num_vals= np.array(list(vals))\n",
    "print(f' biggest value is {np.max(num_vals)}') \n",
    "\n",
    "#print all keys\n",
    "print(mydict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_ml_test_harness(train_x:np.ndarray,train_y:np.ndarray,\n",
    "                          test_x:np.ndarray=None,test_y:np.ndarray=None):\n",
    "    \"\"\" code to compare supervised machine learning algorithms on a dataset\"\"\"\n",
    "    # insert your code below here\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    if test_x is None or test_y is None:\n",
    "        test_x, test_y = train_x, train_y\n",
    "    \n",
    "    for k in [1, 3, 5, 7, 9]:\n",
    "        knn = SimpleKNNClassifier(k=k)\n",
    "        knn.fit(train_x, train_y)\n",
    "        pred_y = knn.predict(test_x)\n",
    "        accuracy = np.mean(pred_y == test_y)\n",
    "        experiment_name = f'KNN_K={k}'\n",
    "        results[experiment_name] = accuracy\n",
    "    \n",
    "    for max_depth in [None, 2, 4, 6]:\n",
    "        for min_samples_split in [2, 4, 6]:\n",
    "            for criterion in ['gini', 'entropy']:\n",
    "                dt = DecisionTreeClassifier(max_depth=max_depth, \n",
    "                                          min_samples_split=min_samples_split,\n",
    "                                          criterion=criterion)\n",
    "                dt.fit(train_x, train_y)\n",
    "                pred_y = dt.predict(test_x)\n",
    "                accuracy = np.mean(pred_y == test_y)\n",
    "                experiment_name = f'DT_depth={max_depth}_split={min_samples_split}_crit={criterion}'\n",
    "                results[experiment_name] = accuracy\n",
    "    \n",
    "    # Report all results\n",
    "    for experiment, accuracy in results.items():\n",
    "        print(f\"{experiment}: {accuracy:.4f}\")\n",
    "    \n",
    "    # Find and report the best performing configuration\n",
    "    best_experiment = max(results, key=results.get)\n",
    "    best_accuracy = results[best_experiment]\n",
    "    print(f\"\\nBest configuration: {best_experiment} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    return results\n",
    "    #insert your code above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN_K=1: 1.0000\n",
      "KNN_K=3: 0.9600\n",
      "KNN_K=5: 0.9600\n",
      "KNN_K=7: 0.9700\n",
      "KNN_K=9: 0.9700\n",
      "DT_depth=None_split=2_crit=gini: 1.0000\n",
      "DT_depth=None_split=2_crit=entropy: 1.0000\n",
      "DT_depth=None_split=4_crit=gini: 0.9800\n",
      "DT_depth=None_split=4_crit=entropy: 0.9900\n",
      "DT_depth=None_split=6_crit=gini: 0.9800\n",
      "DT_depth=None_split=6_crit=entropy: 0.9900\n",
      "DT_depth=2_split=2_crit=gini: 0.9700\n",
      "DT_depth=2_split=2_crit=entropy: 0.9700\n",
      "DT_depth=2_split=4_crit=gini: 0.9700\n",
      "DT_depth=2_split=4_crit=entropy: 0.9700\n",
      "DT_depth=2_split=6_crit=gini: 0.9700\n",
      "DT_depth=2_split=6_crit=entropy: 0.9700\n",
      "DT_depth=4_split=2_crit=gini: 0.9900\n",
      "DT_depth=4_split=2_crit=entropy: 0.9900\n",
      "DT_depth=4_split=4_crit=gini: 0.9800\n",
      "DT_depth=4_split=4_crit=entropy: 0.9900\n",
      "DT_depth=4_split=6_crit=gini: 0.9800\n",
      "DT_depth=4_split=6_crit=entropy: 0.9900\n",
      "DT_depth=6_split=2_crit=gini: 1.0000\n",
      "DT_depth=6_split=2_crit=entropy: 1.0000\n",
      "DT_depth=6_split=4_crit=gini: 0.9800\n",
      "DT_depth=6_split=4_crit=entropy: 0.9900\n",
      "DT_depth=6_split=6_crit=gini: 0.9800\n",
      "DT_depth=6_split=6_crit=entropy: 0.9900\n",
      "\n",
      "Best configuration: KNN_K=1 with accuracy 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN_K=1': np.float64(1.0),\n",
       " 'KNN_K=3': np.float64(0.96),\n",
       " 'KNN_K=5': np.float64(0.96),\n",
       " 'KNN_K=7': np.float64(0.97),\n",
       " 'KNN_K=9': np.float64(0.97),\n",
       " 'DT_depth=None_split=2_crit=gini': np.float64(1.0),\n",
       " 'DT_depth=None_split=2_crit=entropy': np.float64(1.0),\n",
       " 'DT_depth=None_split=4_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=None_split=4_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=None_split=6_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=None_split=6_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=2_split=2_crit=gini': np.float64(0.97),\n",
       " 'DT_depth=2_split=2_crit=entropy': np.float64(0.97),\n",
       " 'DT_depth=2_split=4_crit=gini': np.float64(0.97),\n",
       " 'DT_depth=2_split=4_crit=entropy': np.float64(0.97),\n",
       " 'DT_depth=2_split=6_crit=gini': np.float64(0.97),\n",
       " 'DT_depth=2_split=6_crit=entropy': np.float64(0.97),\n",
       " 'DT_depth=4_split=2_crit=gini': np.float64(0.99),\n",
       " 'DT_depth=4_split=2_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=4_split=4_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=4_split=4_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=4_split=6_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=4_split=6_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=6_split=2_crit=gini': np.float64(1.0),\n",
       " 'DT_depth=6_split=2_crit=entropy': np.float64(1.0),\n",
       " 'DT_depth=6_split=4_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=6_split=4_crit=entropy': np.float64(0.99),\n",
       " 'DT_depth=6_split=6_crit=gini': np.float64(0.98),\n",
       " 'DT_depth=6_split=6_crit=entropy': np.float64(0.99)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now run your code for the iris data\n",
    "first_ml_test_harness(train_x, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
